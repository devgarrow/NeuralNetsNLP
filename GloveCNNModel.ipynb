{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ69pAC5fHeK"
   },
   "source": [
    "# **Setup** (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL_PeLcEb7B-",
    "outputId": "54175b27-d747-4740-fdd9-1777ce608b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-08 16:58:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-11-08 16:58:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-11-08 16:58:07--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.1’\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  2.87MB/s    in 2m 49s  \n",
      "\n",
      "2023-11-08 17:00:58 (4.87 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wy4Kl0ZRcnPS"
   },
   "outputs": [],
   "source": [
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQuCxqSpfR_a"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FXoWhEcIcx6r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "import os\n",
    "import pathlib\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD3ZJ1fleZfm"
   },
   "source": [
    "## **1. Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g3Tp5XH5UpL",
    "outputId": "15f81d7a-c89a-4978-da5e-bf8f3ea655e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', 'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', 'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve 20NewsGroup dataset\n",
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOzSWD1eg8S"
   },
   "source": [
    "## **2. Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8Rte6uH51wV",
    "outputId": "092420ff-2051-4321-e725-6e0327ef2a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "# Data Pre-Processing\n",
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A96JXYHOQnaZ"
   },
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fprQ80zwLGAF"
   },
   "outputs": [],
   "source": [
    "# Model Constants\n",
    "DIMENSIONAL_LSTM_SIZE = 256\n",
    "MAX_NUM_WORDS = 20000 # placeholder -- can just make this the size of input data\n",
    "ENCODED_VECTOR_SIZE = 100 # 50, 100, 200, 300\n",
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMqCcvWueuoD"
   },
   "source": [
    "## **3. Data Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tVuCAZYnP_Yz"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "vectorizer = TextVectorization(max_tokens = MAX_NUM_WORDS, output_sequence_length = MAX_SEQUENCE_LENGTH)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUZaIgiCe0ud"
   },
   "source": [
    "## **4. Pre-Trained Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwwi-9f1Sh0X",
    "outputId": "0bf3d17f-29c3-4490-a19d-c974af5ff44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained word embeddings\n",
    "path_to_glove_file =  \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GR3huZWe67K",
    "outputId": "81990ab5-de81-4d3e-8afb-239f62406510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17999 words (2001 misses)\n"
     ]
    }
   ],
   "source": [
    "# Embedding Matrix\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q_AM727e8cI"
   },
   "source": [
    "# **5. Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJItqLsZ565O",
    "outputId": "5571410b-bdf4-4b24-9e24-7e08ed8a0070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPoolin  (None, None, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2148956 (8.20 MB)\n",
      "Trainable params: 148756 (581.08 KB)\n",
      "Non-trainable params: 2000200 (7.63 MB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Sample CNN Model\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(num_tokens, ENCODED_VECTOR_SIZE, embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False))\n",
    "model.add(Conv1D(128, 5, activation=\"sigmoid\"))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 5, activation=\"sigmoid\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(len(class_names), activation='sigmoid'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu1nZ_MsfBox"
   },
   "source": [
    "## **6. Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iSKb7tFvfGCk"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Rhgv8bm4ku3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 14s 113ms/step - loss: 2.8568 - acc: 0.1173 - val_loss: 2.5078 - val_acc: 0.2073\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 2.2587 - acc: 0.2537 - val_loss: 2.1220 - val_acc: 0.2998\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 17s 137ms/step - loss: 1.9435 - acc: 0.3640 - val_loss: 1.8412 - val_acc: 0.3863\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 1.7212 - acc: 0.4289 - val_loss: 1.6732 - val_acc: 0.4546\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 17s 132ms/step - loss: 1.5659 - acc: 0.4814 - val_loss: 1.5563 - val_acc: 0.4731\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 16s 128ms/step - loss: 1.4431 - acc: 0.5206 - val_loss: 1.4476 - val_acc: 0.5149\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 1.3427 - acc: 0.5566 - val_loss: 1.3784 - val_acc: 0.5419\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 18s 142ms/step - loss: 1.2575 - acc: 0.5838 - val_loss: 1.3044 - val_acc: 0.5671\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 15s 121ms/step - loss: 1.1878 - acc: 0.6071 - val_loss: 1.2572 - val_acc: 0.5709\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 1.1300 - acc: 0.6270 - val_loss: 1.2170 - val_acc: 0.5904\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 1.0803 - acc: 0.6428 - val_loss: 1.2044 - val_acc: 0.5899\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 1.0356 - acc: 0.6618 - val_loss: 1.1745 - val_acc: 0.6097\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.9945 - acc: 0.6748 - val_loss: 1.1717 - val_acc: 0.6134\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.9595 - acc: 0.6823 - val_loss: 1.1447 - val_acc: 0.6069\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 0.9241 - acc: 0.6983 - val_loss: 1.1206 - val_acc: 0.6257\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.8920 - acc: 0.7071 - val_loss: 1.1105 - val_acc: 0.6244\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 16s 125ms/step - loss: 0.8603 - acc: 0.7177 - val_loss: 1.0738 - val_acc: 0.6334\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 16s 127ms/step - loss: 0.8292 - acc: 0.7315 - val_loss: 1.0627 - val_acc: 0.6444\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 15s 123ms/step - loss: 0.8016 - acc: 0.7385 - val_loss: 1.0536 - val_acc: 0.6427\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 16s 126ms/step - loss: 0.7756 - acc: 0.7470 - val_loss: 1.0390 - val_acc: 0.6557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x144079b90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
